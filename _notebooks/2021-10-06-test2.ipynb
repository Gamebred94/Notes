{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Reinforcement Learning\n",
    "> \"Demo and Discussion of RL & OpenAI gym\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "%matplotlib inline\n",
    "import PIL\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('animation', html='jshtml')\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "#from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a nutshell, RL is the study of agents and how they learn by trial and error\n",
    "- It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future\n",
    "\n",
    "This is quite a broad setting, which can apply to a wide variety of tasks.  \n",
    "\n",
    "<img src=\"my_icons/RL_applications.JPG\" width=700 height=700 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending signals to activate motors  \n",
    "1. The agent can be the program controlling Ms. Pac-Man. In this case, the environment is a simulation of the Atari game\n",
    "1. The agent can be the program playing a board game such as Go\n",
    "1. The agent does not have to control a physically (or virtually) moving thing. A smart thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature\n",
    "1. The agent can observe stock market prices and decide how much to buy\n",
    "or sell every second. Rewards are obviously the monetary gains and\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"my_icons/RL_framework.JPG\" width=700 height=700 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts and Terminology**:  \n",
    "\n",
    "1. The main characters of RL are the agent and the environment:<br>\n",
    "   - The agent is an entity which is of interest to us\n",
    "   - The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the environment, and then decides on an action to take  \n",
    "   - The environment changes when the agent acts on it, but may also change on its own  \n",
    "\n",
    "2. <br></br> \n",
    "   - The agent also perceives a reward signal from the environment, a number that tells it how good or bad the current world state is  \n",
    "   - The goal of the agent is to maximize its cumulative reward, called return. Reinforcement learning    methods are ways that the agent can learn behaviors to achieve its goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> To talk more specifically what RL does, we need to introduce additional terminology. We need to talk about states and observations,\n",
    "action spaces,\n",
    "policies,\n",
    "return,\n",
    "and value functions."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
