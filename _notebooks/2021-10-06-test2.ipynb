{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Reinforcement Learning\n",
    "> \"Demo and Discussion of RL & OpenAI gym\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "%matplotlib inline\n",
    "import PIL\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('animation', html='jshtml')\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "#from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a nutshell, RL is the study of agents and how they learn by trial and error\n",
    "- It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future\n",
    "\n",
    "This is quite a broad setting, which can apply to a wide variety of tasks.  \n",
    "\n",
    "<img src=\"my_icons/RL_applications.JPG\" width=700 height=700 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending signals to activate motors  \n",
    "1. The agent can be the program controlling Ms. Pac-Man. In this case, the environment is a simulation of the Atari game\n",
    "1. The agent can be the program playing a board game such as Go\n",
    "1. The agent does not have to control a physically (or virtually) moving thing. A smart thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature\n",
    "1. The agent can observe stock market prices and decide how much to buy\n",
    "or sell every second. Rewards are obviously the monetary gains and\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"my_icons/RL_framework.JPG\" width=700 height=700 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts and Terminology**:  \n",
    "\n",
    "1. The main characters of RL are the agent and the environment:<br>\n",
    "   - The agent is an entity which is of interest to us\n",
    "   - The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the environment, and then decides on an action to take  \n",
    "   - The environment changes when the agent acts on it, but may also change on its own  \n",
    "\n",
    "2. Moreover:\n",
    "   - The agent perceives a reward signal from the environment, a number that tells it how good or bad the current world state is  \n",
    "   - The goal of the agent is to maximize its cumulative reward, called return. Reinforcement learning    methods are ways that the agent can learn behaviors to achieve its goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> To talk more specifically what RL does, we need to introduce additional terminology. We need to talk about states and observations,\n",
    "action spaces,\n",
    "policies,\n",
    "return,\n",
    "and value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States and Observations<br>\n",
    "- A state '**s**' is a complete description of the state of the world.\n",
    "There is no information about the world which is hidden from the state.  \n",
    "- An observation o is a partial description of a state, which may omit information. \n",
    "A lot of the times, observations and state are used interchangeably\n",
    "\n",
    "- In deep RL, we almost always represent states and observations by a real-valued vector, matrix, or higher-order tensor\n",
    "For instance, a visual observation could be represented by the RGB matrix of its pixel values;   \n",
    "the state of a robot might be represented by its joint angles and velocities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Spaces <br>\n",
    "- Different environments allow different kinds of actions  \n",
    "- The set of all valid actions in a given environment is often called the action space. Some environments, like Atari and Go, have discrete action spaces, where only a finite number of moves are available to the agent  \n",
    "Other environments, like where the agent controls a robot in a physical world, have continuous action spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "- The algorithm a software agent uses to determine its actions is called its policy. It can be deterministic or stochastic denoted by $\\Pi$\n",
    "- Because the policy is essentially the agent’s brain, it’s not uncommon to substitute the word “policy” for “agent”, eg saying “The policy is trying to maximize reward.”\n",
    "- In deep RL, we deal with parameterized policies i.e policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm  \n",
    "\n",
    "<img src=\"my_icons/RL_agent.JPG\" width=800 height=800 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy design<br>\n",
    "\n",
    "- Techniques to design a policy:\n",
    "\n",
    "      For example, consider a robotic vacuum cleaner whose reward is\n",
    "      the amount of dust it picks up in 30 minutes. \n",
    "      Its policy could be to move forward with some probability p every second,\n",
    "      or randomly rotate left or right with probability 1 – p. \n",
    "      The rotation angle would be a random angle between –r and +r.\n",
    "      \n",
    "      How would you train such a robot?\n",
    "      There are just two policy parameters you can tweak: the probability p and the angle range r.\n",
    "\n",
    "<img src=\"my_icons/RL_vaccum_robot.JPG\" width=700 height=700 left   align=\"left\"/>.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four points in policy space (left) and the agent’s corresponding behavior (right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One possible learning algorithm could be to try out many different values for these parameters, and pick the\n",
    "combination that performs best (see above Figure).  \n",
    "This is an example of ***policy search using a brute force approach***. When the policy space is too\n",
    "large (which is generally the case), finding a good set of parameters  \n",
    "this way is like searching for a needle in a gigantic haystack.\n",
    "- Another way to explore the policy space is to use genetic algorithms. For example, you could randomly create a first generation of 100 policies and try them out,  \n",
    "then “kill” the 80 worst policies and make the 20 survivors produce 4 offspring each.  \n",
    "An offspring is a copy of its parent plus some random variation. The surviving policies plus their offspring together constitute the second generation.  \n",
    "we can continue to iterate through generations this way until we find a good policy.\n",
    "- Yet another approach is to use optimization techniques, by evaluating the\n",
    "gradients of the rewards with regard to the policy parameters,  \n",
    "then tweaking these parameters by following the gradients toward higher rewards.  \n",
    "This approach, is called **policy gradients (PG)** .  \n",
    "Going back to the vacuum cleaner robot, you could slightly increase p and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes;  \n",
    "if it does, then increase p some more, or else reduce p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
