{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Reinforcement Learning\n",
    "> \"Demo and Discussion of RL & OpenAI gym\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%matplotlib inline\n",
    "import PIL\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('animation', html='jshtml')\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "#from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a nutshell, RL is the study of agents and how they learn by trial and error\n",
    "- It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future\n",
    "\n",
    "This is quite a broad setting, which can apply to a wide variety of tasks.  \n",
    "\n",
    "<img src=\"my_icons/RL_applications.JPG\" width=700 height=700 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending signals to activate motors  \n",
    "1. The agent can be the program controlling Ms. Pac-Man. In this case, the environment is a simulation of the Atari game\n",
    "1. The agent can be the program playing a board game such as Go\n",
    "1. The agent does not have to control a physically (or virtually) moving thing. A smart thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature\n",
    "1. The agent can observe stock market prices and decide how much to buy\n",
    "or sell every second. Rewards are obviously the monetary gains and\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"my_icons/RL_framework.JPG\" width=700 height=700 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts and Terminology**:  \n",
    "\n",
    "1. The main characters of RL are the agent and the environment:<br>\n",
    "   - The agent is an entity which is of interest to us\n",
    "   - The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the environment, and then decides on an action to take  \n",
    "   - The environment changes when the agent acts on it, but may also change on its own  \n",
    "\n",
    "2. Moreover:\n",
    "   - The agent perceives a reward signal from the environment, a number that tells it how good or bad the current world state is  \n",
    "   - The goal of the agent is to maximize its cumulative reward, called return. Reinforcement learning    methods are ways that the agent can learn behaviors to achieve its goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> To talk more specifically what RL does, we need to introduce additional terminology. We need to talk about states and observations,\n",
    "action spaces,\n",
    "policies,\n",
    "return,\n",
    "and value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States and Observations<br>\n",
    "- A state '**s**' is a complete description of the state of the world.\n",
    "There is no information about the world which is hidden from the state.  \n",
    "- An observation o is a partial description of a state, which may omit information. \n",
    "A lot of the times, observations and state are used interchangeably\n",
    "\n",
    "- In deep RL, we almost always represent states and observations by a real-valued vector, matrix, or higher-order tensor\n",
    "For instance, a visual observation could be represented by the RGB matrix of its pixel values;   \n",
    "the state of a robot might be represented by its joint angles and velocities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Spaces <br>\n",
    "- Different environments allow different kinds of actions  \n",
    "- The set of all valid actions in a given environment is often called the action space. Some environments, like Atari and Go, have discrete action spaces, where only a finite number of moves are available to the agent  \n",
    "Other environments, like where the agent controls a robot in a physical world, have continuous action spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "- The algorithm a software agent uses to determine its actions is called its policy. It can be deterministic or stochastic denoted by $\\Pi$\n",
    "- Because the policy is essentially the agent’s brain, it’s not uncommon to substitute the word “policy” for “agent”, eg saying “The policy is trying to maximize reward.”\n",
    "- In deep RL, we deal with parameterized policies i.e policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm  \n",
    "\n",
    "<img src=\"my_icons/RL_agent.JPG\" width=800 height=800 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy design<br>\n",
    "\n",
    "- Techniques to design a policy:\n",
    "\n",
    "      For example, consider a robotic vacuum cleaner whose reward is\n",
    "      the amount of dust it picks up in 30 minutes. \n",
    "      Its policy could be to move forward with some probability p every second,\n",
    "      or randomly rotate left or right with probability 1 – p. \n",
    "      The rotation angle would be a random angle between –r and +r.\n",
    "      \n",
    "      How would you train such a robot?\n",
    "      There are just two policy parameters you can tweak: the probability p and the angle range r.\n",
    "\n",
    "<img src=\"my_icons/RL_vaccum_robot.JPG\" width=700 height=700 left   align=\"left\"/>.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four points in policy space (left) and the agent’s corresponding behavior (right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One possible learning algorithm could be to try out many different values for these parameters, and pick the\n",
    "combination that performs best (see above Figure).  \n",
    "This is an example of ***policy search using a brute force approach***. When the policy space is too\n",
    "large (which is generally the case), finding a good set of parameters  \n",
    "this way is like searching for a needle in a gigantic haystack.\n",
    "- Another way to explore the policy space is to use genetic algorithms. For example, you could randomly create a first generation of 100 policies and try them out,  \n",
    "then “kill” the 80 worst policies and make the 20 survivors produce 4 offspring each.  \n",
    "An offspring is a copy of its parent plus some random variation. The surviving policies plus their offspring together constitute the second generation.  \n",
    "we can continue to iterate through generations this way until we find a good policy.\n",
    "- Yet another approach is to use optimization techniques, by evaluating the\n",
    "gradients of the rewards with regard to the policy parameters, then tweaking these parameters by following the gradients toward higher rewards. This approach, is called **policy gradients (PG)** .  \n",
    "\n",
    "Going back to the vacuum cleaner robot, you could slightly increase p and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes;  \n",
    "if it does, then increase p some more, or else reduce p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">OpenAI Gym is a toolkit that provides a wide variety of simulated\n",
    "environments(Atari games, board games, 2D and 3D physical simulations etc.) so you can train agents, compare them, or develop new RL algorithms.<br>  \n",
    ">One of the challenges of Reinforcement Learning is that in order to train an\n",
    "agent, you first need to have a working environment.\n",
    "If you want to program an agent that will learn to play an Atari game, you will need an Atari game\n",
    "simulator. If you want to program a walking robot, then the environment is the\n",
    "real world, and you can directly train your robot in that environment, but this has\n",
    "its limits: if the robot falls off a cliff, you can’t just click Undo. \n",
    "You can’t speed up time either; \n",
    "And it’s generally too expensive to train 1,000 robots in parallel.   \n",
    "In short, training is hard and slow in the real world, so you generally need a simulated environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_28314e7a_2776_11ec_ada1_cc2f716aa8ff th {\n",
       "          font-size: 20pt;\n",
       "    }#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow0_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow1_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow2_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow3_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow4_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow5_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow6_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow7_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow8_col0,#T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow9_col0{\n",
       "            font-size:  20pt;\n",
       "        }</style><table id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ff\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >OpenAI_Environments</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row0\" class=\"row_heading level0 row0\" >3</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow0_col0\" class=\"data row0 col0\" >ReversedAddition3-v0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row1\" class=\"row_heading level0 row1\" >31</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow1_col0\" class=\"data row1 col0\" >Striker-v2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow2_col0\" class=\"data row2 col0\" >RepeatCopy-v0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row3\" class=\"row_heading level0 row3\" >47</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow3_col0\" class=\"data row3 col0\" >FetchSlide-v1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row4\" class=\"row_heading level0 row4\" >6</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow4_col0\" class=\"data row4 col0\" >CartPole-v0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row5\" class=\"row_heading level0 row5\" >11</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow5_col0\" class=\"data row5 col0\" >Acrobot-v1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row6\" class=\"row_heading level0 row6\" >27</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow6_col0\" class=\"data row6 col0\" >HotterColder-v0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row7\" class=\"row_heading level0 row7\" >48</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow7_col0\" class=\"data row7 col0\" >FetchPickAndPlace-v1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row8\" class=\"row_heading level0 row8\" >40</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow8_col0\" class=\"data row8 col0\" >Walker2d-v2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8fflevel0_row9\" class=\"row_heading level0 row9\" >19</th>\n",
       "                        <td id=\"T_28314e7a_2776_11ec_ada1_cc2f716aa8ffrow9_col0\" class=\"data row9 col0\" >KellyCoinflipGeneralized-v0</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x175dfe7b288>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({\"OpenAI_Environments\": [i.id for i in list(gym.envs.registry.all())[:50]]}).sample(10)\n",
    "(df.style\n",
    "        .set_table_styles([{'selector': 'th', 'props': [('font-size', '20pt')]}])\n",
    "        .set_properties(**{'font-size': '20pt'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cart Pole/Inverted pendulum problem<br>\n",
    "\n",
    "The Cart-Pole is a classic control problem. It is a very simple environment composed of a cart that can move left or right, and pole placed vertically on top of it.   \n",
    "The agent must move the cart left or right to keep the pole upright.<br>  \n",
    "\n",
    "> youtube: https://youtu.be/qMlcsc43-lg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"my_icons/Cartpole.JPG\" width=700 height=700 left   align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations vary depending on the environment. In this case it is a 1D NumPy array composed of 4 floats:   \n",
    "they represent the cart’s horizontal position (0.0 = center), its velocity (positive means right), the angle of the pole, and its angular velocity\n",
    "(positive means clockwise) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(42)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide  \n",
    "An environment can be visualized by calling its render() method, and you can pick the rendering mode (the rendering options depend on the environment)\n",
    "In this example we will set mode=\"rgb_array\" to get an image of the environment as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAGCCAYAAADkJxkCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJpElEQVR4nO3dz4ucdx3A8e9kJ8kmNDYhRmjFqhRK9WDJQUW09ei5CLkU/BsEr/ofeJNePMUWNBAR8WLxJtpDqaI9SaEoKLZGarWbmmy33Xm8SNG2mUjdmUne83rBwvI83535sLBf3vNrn9k0TQMAoOzYpgcAAFg1wQMA5AkeACBP8AAAeYIHAMgTPABA3vw2531mHQC4W8xudcIzPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgb77pAYDtMS0O3/l+dmxng5MA20bwAGvzwlPfGIcHN8fs2Hw88rVvv++a2c7xcWzH1gQcLbsKsHbT4u3x28tff99zDzz6xLjwqcfWOxCQ5z08AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBA6zFS888OQ7f2l+65sMPf2mc/cTFNU0EbBPBA6zFzdf+PMY0LV1z/PS94/ipM2uaCNgmggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIED7Byb1z7/VgcHmx6DGCLCR5g5f707A/G2zevL11z8kMfGafOf2xNEwHbRvAAd4Qz9z80zn3y4qbHAKIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeYKWmafofV85WOgew3QQPsFIvPfOdcePVPy5dc+/HPzMeePSJNU0EbCPBA9wRZjPP8ACrI3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHWJlXfvPT8c9rf1i6ZvfcfeOjn3t8TRMB20rwACvz5ut/HYcHN5au2Tm+O06du39NEwHbSvAAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR7gHXt7e2M+nx/Z1+XvXb7tfT733HNHcl9Xr15d/S8IuGvNNz0AcGc5PDw8stuaFtPt10zTkdznYrH4v28D6BI8wEq9tTgxFtPOu45O4+TO/kbmAbaT4AFWZn9xejz/96+M1w7u+6/js3E4vnzhh+Oe+esbmgzYNt7DA6zMi9c/+57YGWOMaeyMX776+PjHwYUNTAVsI8EDrMSD958b950/c8vzb08nxrPXHhu/evHlNU4FbCvBA6zE5y9+cTz84KeXrvnb3s3x5I+fX9NEwDYTPMBKLKadMdligDuE3QhYifMnXh5n5q9tegyAMYbgATZkPjsYj9z7802PAWwJwQOszEP3/HqcO/6X9xyfjcPxhfM/GedPvrKBqYBt5P/wACvx1M9eGD/6xe/Gm4dPj8W/t5rvf+ur457dE2OMMU7P3xjXNzkgsFUED7AS128cjOs3DsYYb7xz7NI3vztm/7FmMd3+0hMAR2E2Ldlwdnd37UawRaZpGgcHB5se4wOZz+djZ+fdl7AAtsn+/v7sVueWBs+07CSQs7e3N86ePbvpMT6QK1eujEuXLm16DGCDZrPZLYNn6UtaS34OCLrb/+bv9vmB1fEpLQAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMhbei0tYPvM53fntnDsmMdvwK0tvVr6GMPV0gGAu8UtryDsIREAkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkCR4AIE/wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeYIHAMib3+b8bC1TAACskGd4AIA8wQMA5AkeACBP8AAAeYIHAMgTPABA3r8AOfjBfJ8Qw9wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "def plot_environment(env, figsize=(10,8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img\n",
    "\n",
    "plot_environment(env)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
