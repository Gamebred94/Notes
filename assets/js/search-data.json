{
  
    
        "post0": {
            "title": "An introduction to Reinforcement Learning",
            "content": "%matplotlib inline import PIL import gym import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # To get smooth animations import matplotlib.animation as animation import matplotlib.pyplot as plt mpl.rc(&#39;animation&#39;, html=&#39;jshtml&#39;) # Imports specifically so we can render outputs in Jupyter. #from JSAnimation.IPython_display import display_animation from matplotlib import animation from IPython.display import display import tensorflow as tf import pandas as pd from tensorflow import keras . . What is RL? . In a nutshell, RL is the study of agents and how they learn by trial and error | It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future | . This is quite a broad setting, which can apply to a wide variety of tasks. . . The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending signals to activate motors | The agent can be the program controlling Ms. Pac-Man. In this case, the environment is a simulation of the Atari game | The agent can be the program playing a board game such as Go | The agent does not have to control a physically (or virtually) moving thing. A smart thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature | The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses | RL framework . . Key Concepts and Terminology: . The main characters of RL are the agent and the environment: . The agent is an entity which is of interest to us | The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the environment, and then decides on an action to take | The environment changes when the agent acts on it, but may also change on its own | . | Moreover: . The agent perceives a reward signal from the environment, a number that tells it how good or bad the current world state is | The goal of the agent is to maximize its cumulative reward, called return. Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal | . | To talk more specifically what RL does, we need to introduce additional terminology. We need to talk about states and observations, action spaces, policies, return, and value functions. . States and Observations . A state &#39;s&#39; is a complete description of the state of the world. There is no information about the world which is hidden from the state. | An observation o is a partial description of a state, which may omit information. A lot of the times, observations and state are used interchangeably . | In deep RL, we almost always represent states and observations by a real-valued vector, matrix, or higher-order tensor For instance, a visual observation could be represented by the RGB matrix of its pixel values; the state of a robot might be represented by its joint angles and velocities . | . Action Spaces . Different environments allow different kinds of actions | The set of all valid actions in a given environment is often called the action space. Some environments, like Atari and Go, have discrete action spaces, where only a finite number of moves are available to the agent Other environments, like where the agent controls a robot in a physical world, have continuous action spaces | . Policies . The algorithm a software agent uses to determine its actions is called its policy. It can be deterministic or stochastic denoted by $ Pi$ | Because the policy is essentially the agent’s brain, it’s not uncommon to substitute the word “policy” for “agent”, eg saying “The policy is trying to maximize reward.” | In deep RL, we deal with parameterized policies i.e policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm | . . Policy design . Techniques to design a policy: . For example, consider a robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability p every second, or randomly rotate left or right with probability 1 – p. The rotation angle would be a random angle between –r and +r. How would you train such a robot? There are just two policy parameters you can tweak: the probability p and the angle range r. . | . . . Four points in policy space (left) and the agent’s corresponding behavior (right) . One possible learning algorithm could be to try out many different values for these parameters, and pick the combination that performs best (see above Figure). This is an example of policy search using a brute force approach. When the policy space is too large (which is generally the case), finding a good set of parameters this way is like searching for a needle in a gigantic haystack. | Another way to explore the policy space is to use genetic algorithms. For example, you could randomly create a first generation of 100 policies and try them out, then “kill” the 80 worst policies and make the 20 survivors produce 4 offspring each. An offspring is a copy of its parent plus some random variation. The surviving policies plus their offspring together constitute the second generation. we can continue to iterate through generations this way until we find a good policy. | Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regard to the policy parameters, then tweaking these parameters by following the gradients toward higher rewards. This approach, is called policy gradients (PG) . | . Going back to the vacuum cleaner robot, you could slightly increase p and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes; if it does, then increase p some more, or else reduce p. . OpenAI Gym . OpenAI Gym is a toolkit that provides a wide variety of simulated environments(Atari games, board games, 2D and 3D physical simulations etc.) so you can train agents, compare them, or develop new RL algorithms. One of the challenges of Reinforcement Learning is that in order to train an agent, you first need to have a working environment. If you want to program an agent that will learn to play an Atari game, you will need an Atari game simulator. If you want to program a walking robot, then the environment is the real world, and you can directly train your robot in that environment, but this has its limits:if the robot falls off a cliff, you can’t just click Undo. You can’t speed up time either; And it’s generally too expensive to train 1,000 robots in parallel. In short, training is hard and slow in the real world, so you generally need a simulated environment. . df = pd.DataFrame({&quot;OpenAI_Environments&quot;: [i.id for i in list(gym.envs.registry.all())[:50]]}).sample(10) (df.style .set_table_styles([{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;20pt&#39;)]}]) .set_properties(**{&#39;font-size&#39;: &#39;20pt&#39;}) ) . . OpenAI_Environments . 3 ReversedAddition3-v0 | . 31 Striker-v2 | . 1 RepeatCopy-v0 | . 47 FetchSlide-v1 | . 6 CartPole-v0 | . 11 Acrobot-v1 | . 27 HotterColder-v0 | . 48 FetchPickAndPlace-v1 | . 40 Walker2d-v2 | . 19 KellyCoinflipGeneralized-v0 | . The Cart Pole/Inverted pendulum problem . The Cart-Pole is a classic control problem. It is a very simple environment composed of a cart that can move left or right, and pole placed vertically on top of it. The agent must move the cart left or right to keep the pole upright. . .",
            "url": "https://gamebred94.github.io/reinforcement-learning/2021/10/06/test2.html",
            "relUrl": "/2021/10/06/test2.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gamebred94.github.io/reinforcement-learning/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gamebred94.github.io/reinforcement-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gamebred94.github.io/reinforcement-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}